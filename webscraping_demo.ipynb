{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPkP9cUT/TM0TjspqSzti0u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chyylee/PythonDemos/blob/main/webscraping_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Webscraping From clinicaltrials.gov\n",
        "Recently we had a company give a talk on what data analysis can look like in a typical life science consulting project. One area that came up that likely was not common knowledge for our consultants is a concept called webscraping.\n",
        "\n",
        "In simple terms, webscraping describes extracting data from websites which can be useful when trying to pull large amounts of data from online repositories or databases.\n",
        "\n",
        "A common technique is to use a programming language called python, which has recently been integrated into google suites in google collab.\n",
        "\n",
        "Below I will walk through an example of how to webscrape from clinicaltrials.gov and save that data to be analyzed in excel or other programming languages.\n",
        "\n"
      ],
      "metadata": {
        "id": "oos7vPZFvSXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Webscraping Basics"
      ],
      "metadata": {
        "id": "ezo5F7vqwOgU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python is flexible and can import packages, libraries, and/or modules that make accomplishing certain tasks easier, bellow we import several packages/modules to use for webscraping and \n",
        "data analysis."
      ],
      "metadata": {
        "id": "pVdkgXUnx5i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4 # for accessing web data\n",
        "from collections import defaultdict # data processing\n",
        "from bs4 import BeautifulSoup  # accessing web data\n",
        "import requests # accessing web data \n",
        "import pandas as pd # data processing\n"
      ],
      "metadata": {
        "id": "xdx-b9uywSIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A unique module we will need is to orient our code to be able to write and read files from google drive (not needed if you use python locally on your computer). "
      ],
      "metadata": {
        "id": "C2UpGD3Qxk20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ],
      "metadata": {
        "id": "8SZMSIEGxwt6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69669789-cd1b-4981-9671-2b8f1b01c48a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After initiliazing these modules, we can write a function that can take in\n",
        "a NCTid for a clinical trial."
      ],
      "metadata": {
        "id": "ow_lfT0oyEPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write function that intakes a NCT ID from Clinicaltrials.gov\n",
        "def clinicalTrialsGov(nctid,printflag = False):\n",
        "    data = defaultdict(list) # presets data entry (into dictionary as list)\n",
        "\n",
        "    # calls beautiful soup to interact with website API\n",
        "    soup = BeautifulSoup(requests.get(\"https://clinicaltrials.gov/ct2/show/\" + \n",
        "                                      nctid + \"?displayxml=true\").text, \"xml\") \n",
        "\n",
        "    # Select Information To Collect\n",
        "    subset = ['intervention_type', 'study_type', 'allocation', \n",
        "              'intervention_model', 'primary_purpose', 'masking', \n",
        "              'enrollment', 'official_title', 'condition', 'minimum_age', \n",
        "              'maximum_age', 'gender', 'healthy_volunteers', 'phase', \n",
        "              'primary_outcome', 'secondary_outcome', 'number_of_arms',\n",
        "              'nct_id']\n",
        "\n",
        "    # Loops through all the information defined in subset and pulls the information to store in data\n",
        "    for tag in soup.find_all(subset):\n",
        "        data['ct{}'.format(tag.name.capitalize())].append(tag.get_text(strip=True))\n",
        "\n",
        "    # Prints the data if printflag is True, default is False\n",
        "    if printflag:\n",
        "      for key in data:\n",
        "        print('{}: {}'.format(key, ', '.join(data[key])))\n",
        "\n",
        "        # modifies dictionary to be saved into a dataframe\n",
        "    for key in data:\n",
        "      if len(data[key]) > 1:\n",
        "        lst = data[key]\n",
        "        s = \",\".join(lst)\n",
        "        data[key] = s\n",
        "    \n",
        "    # saves the infromation in dataframe\n",
        "    df = pd.DataFrame(data)\n",
        "    return df"
      ],
      "metadata": {
        "id": "QMetamQBwSwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can input a NCT id of interest and quickly view the contents we denoted we were intersted in."
      ],
      "metadata": {
        "id": "xd9hKbLgyM_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dout = clinicalTrialsGov('NCT02170532',True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIpcs5RPyazF",
        "outputId": "f29398e5-8aea-40fa-f0d2-e4e2ba905c5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ctNct_id: NCT02170532\n",
            "ctOfficial_title: Aerosolized Beta-Agonist Isomers in Asthma\n",
            "ctPhase: Phase 4\n",
            "ctStudy_type: Interventional\n",
            "ctAllocation: Non-Randomized\n",
            "ctIntervention_model: Crossover Assignment\n",
            "ctPrimary_purpose: Treatment\n",
            "ctMasking: None (Open Label)\n",
            "ctPrimary_outcome: Change in Maximum Forced Expiratory Volume at One Second (FEV1)Baseline (before treatment), 30 minutes, 1, 2, 4, 6, and 8 hours post treatment\n",
            "ctSecondary_outcome: Change in 8 Hour Area-under-the-curve FEV10 to 8 hours post dose, Change in Heart RateBaseline (before treatment), 30 minutes, 1, 2, 4, 6, and 8 hours post treatment, Change in Tremor Assessment Measured by a ScaleBaseline (before treatment), 30 minutes, 1, 2, 4, 6, and 8 hours post treatmentTremor assessment will be made on outstretched hands (0 = none, 1+ = fine tremor, barely perceptible, 2+ = obvious tremor)., Change in Dyspnea Response as Measured by the University of California, San Diego (UCSD) Dyspnea ScaleBaseline (before treatment), 30 minutes, 1, 2, 4, 6, and 8 hours post treatment\n",
            "ctNumber_of_arms: 5\n",
            "ctEnrollment: 10\n",
            "ctCondition: Asthma\n",
            "ctIntervention_type: Drug, Drug, Other, Device, Device, Drug\n",
            "ctGender: All\n",
            "ctMinimum_age: 18 Years\n",
            "ctMaximum_age: N/A\n",
            "ctHealthy_volunteers: No\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pulling NCT id's for a given medical condition or treatment\n",
        "Often webscraping is most beneficial when there are set criteria that we would like to pull across a lot of different studies. For example, what if we wanted to pull the top 100 search results for clinical trials pertaining to COVID-19.\n",
        "\n",
        "Using API's set-up by clinicaltrials.gov, this is relatively easy to implement in python."
      ],
      "metadata": {
        "id": "sh4JZveRyfln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get NCT id's for a certain medical condition.\n",
        "# condition = \"covid\", and numstud is the number of studies to return\n",
        "\n",
        "# aside: xml is harder to understand than json - I'll do a json example later\n",
        "def getStudyNCT(condition,numstud):\n",
        "  base_api = 'https://www.clinicaltrials.gov/api/query/full_studies?expr='\n",
        "  out_NCT = []\n",
        "  url = base_api + condition.replace(\" \", \"+\") + '&min_rnk=1&max_rnk=' + str(numstud) + '&fmt=xml'\n",
        "  response = requests.get(url)\n",
        "  soup = BeautifulSoup(response.content, 'lxml')\n",
        "  study_list = soup.find_all(\"fullstudy\")\n",
        "\n",
        "  for study in study_list:\n",
        "    nctid = study.find(\"field\", {\"name\" : \"NCTId\"})\n",
        "    tmp = str(nctid)\n",
        "    data = tmp.split('>')[1].split('<')[0]\n",
        "    out_NCT.append(data)\n",
        "  \n",
        "  return out_NCT"
      ],
      "metadata": {
        "id": "-_eVS97TyfKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can call the above function and get a list of studies to compile information on the study design and status. Here we will just look at the first 10 results of the search."
      ],
      "metadata": {
        "id": "5MnmWquEzVuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NCT_lst = getStudyNCT('covid vaccine',10)\n",
        "NCT_lst"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3asxx46Oze5d",
        "outputId": "f49c0864-76ea-49b8-c734-a296cb4ca0a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NCT05387343',\n",
              " 'NCT05208983',\n",
              " 'NCT04817657',\n",
              " 'NCT05130320',\n",
              " 'NCT05256602',\n",
              " 'NCT04834726',\n",
              " 'NCT05258760',\n",
              " 'NCT05060354',\n",
              " 'NCT04751734',\n",
              " 'NCT05057936']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Going through each of these one at a time and compiling the data we wish to extract from each study would be cumbersome. Luckily, it is pretty easy to loop through a list of NCT codes and save that informatin into a .csv for further analysis in excel."
      ],
      "metadata": {
        "id": "Y5PV0fNIz1Yc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Putting it all together: Scraping data for probiotics\n",
        "We will pull the top 100 results returned for clinical trials pertaining to probiotics and save this information into a .csv file."
      ],
      "metadata": {
        "id": "19KxPH1c0H5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Call `getStudyNCT` to get a list of NCT ids for probiotic clinical trials"
      ],
      "metadata": {
        "id": "sbpoNsRH08Xi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NCT_lst = getStudyNCT('probiotics',100)\n",
        "NCT_lst[0:10] # preview first 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NnZxVo-0fpN",
        "outputId": "c6a97877-718f-49ef-c60b-083c3d0b7dad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NCT03330678',\n",
              " 'NCT01648075',\n",
              " 'NCT01445704',\n",
              " 'NCT05032027',\n",
              " 'NCT02650869',\n",
              " 'NCT05389033',\n",
              " 'NCT04175392',\n",
              " 'NCT02589964',\n",
              " 'NCT05316064',\n",
              " 'NCT04050189']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Call `clinicalTrialsGov` to then process that results for each trial.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FeRGM5X916SB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loop through generated list\n",
        "c = 1\n",
        "for ncid in NCT_lst: \n",
        "  if c == 1:\n",
        "    df = clinicalTrialsGov(ncid) # use first NCTid to initialize the dataframe\n",
        "  else:\n",
        "    out = clinicalTrialsGov(ncid) \n",
        "    df = df.append(out) # append next NCTid to dataframe\n",
        "  c = c + 1"
      ],
      "metadata": {
        "id": "MjCm4pdT2HqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can preview the compiled data and save as a .csv. After running the code below, check you Colab Notebooks folder for the file."
      ],
      "metadata": {
        "id": "CtdkKei95lMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head() # preview the results\n",
        "# Save to a csv file\n",
        "condit = 'probiotics'\n",
        "fnm = \"/drive/My Drive/Colab Notebooks/\" + condit + \"_data.csv\"\n",
        "df.to_csv(fnm)\n"
      ],
      "metadata": {
        "id": "es0xCSwo25ZS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}